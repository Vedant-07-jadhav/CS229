{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc9f8a7",
   "metadata": {},
   "source": [
    "# Using Numpy only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f3ec3",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54611b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression():\n",
    "    def __init__(self, eps=1e-6):\n",
    "        self.eps = eps\n",
    "\n",
    "    def fit(self, x, y, lr=0.01, add_intercept=True):\n",
    "        if add_intercept:\n",
    "            x = np.hstack([np.ones((x.shape[0], 1)), x])\n",
    "        m, n = x.shape\n",
    "\n",
    "        self.theta = np.zeros(n)\n",
    "\n",
    "        while True:\n",
    "            theta_old = self.theta.copy()\n",
    "            h_x = x.dot(self.theta)\n",
    "            grad = (x.T.dot(h_x - y)) / m\n",
    "            self.theta -= lr * grad\n",
    "\n",
    "            if np.linalg.norm(self.theta - theta_old, ord=1) < self.eps:\n",
    "                break\n",
    "\n",
    "    def predict(self, x, add_intercept=True):\n",
    "        if add_intercept:\n",
    "            x = np.hstack([np.ones((x.shape[0], 1)), x])\n",
    "        return x.dot(self.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c10e4c7",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c01b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression():\n",
    "    def __init__(self, eps=1e-6):\n",
    "        self.eps = eps\n",
    "\n",
    "    def fit(self, x, y, add_intercept=True, lr=0.01, max_iter=10000):\n",
    "        if add_intercept:\n",
    "            x = np.hstack([np.ones((x.shape[0], 1)), x])\n",
    "\n",
    "        m, n = x.shape\n",
    "        self.theta = np.zeros(n)\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            theta_old = self.theta.copy()\n",
    "            z = np.clip(x.dot(self.theta), -500, 500)\n",
    "            h_x = 1 / (1 + np.exp(-z))\n",
    "            grad = (x.T.dot(h_x - y)) / m\n",
    "            self.theta -= lr * grad\n",
    "\n",
    "            if np.linalg.norm(self.theta - theta_old, ord=1) < self.eps:\n",
    "                break\n",
    "\n",
    "    def predict_proba(self, x, add_intercept=True):\n",
    "        if add_intercept:\n",
    "            x = np.hstack([np.ones((x.shape[0], 1)), x])\n",
    "        z = np.clip(x.dot(self.theta), -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def predict(self, x, add_intercept=True):\n",
    "        return (self.predict_proba(x, add_intercept) >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31831fff",
   "metadata": {},
   "source": [
    "# Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a48e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2544b",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "922b4e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "x = torch.linspace(0, 10, 100).view(-1, 1)  \n",
    "y = 2 * x + 3 + torch.randn_like(x) \n",
    "\n",
    "x_numpy = x.numpy()\n",
    "y_numpy = y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf530a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 246.7126\n",
      "Epoch [101/1000], Loss: 1.6820\n",
      "Epoch [201/1000], Loss: 1.2833\n",
      "Epoch [301/1000], Loss: 1.1359\n",
      "Epoch [401/1000], Loss: 1.0814\n",
      "Epoch [501/1000], Loss: 1.0613\n",
      "Epoch [601/1000], Loss: 1.0538\n",
      "Epoch [701/1000], Loss: 1.0511\n",
      "Epoch [801/1000], Loss: 1.0501\n",
      "Epoch [901/1000], Loss: 1.0497\n"
     ]
    }
   ],
   "source": [
    "class LinearRegression_pytorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression_pytorch, self).__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = LinearRegression_pytorch()\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 1000\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    if epoch%100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faba8fa",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2efb7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression_pytorch(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1):\n",
    "        super(LogisticRegression_pytorch, self).__init__()\n",
    "        self.linear = nn.Linear(output_dim,input_dim)\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "model = LogisticRegression_pytorch(input_dim=x.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "## now use training loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d94a6",
   "metadata": {},
   "source": [
    "## GDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7bb8700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GDA:\n",
    "    def __init__(self, eps=1e-16):\n",
    "        self.eps = eps\n",
    "        self.phi = None\n",
    "        self.mu0 = None\n",
    "        self.mu1 = None\n",
    "        self.sigma = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        m, n = X.shape\n",
    "        Y = Y.flatten()\n",
    "\n",
    "        count_y1 = np.sum(Y == 1)\n",
    "        count_y0 = m - count_y1\n",
    "        \n",
    "        self.phi = count_y1 / m\n",
    "        \n",
    "        X0 = X[Y == 0]\n",
    "        X1 = X[Y == 1]\n",
    "        \n",
    "        self.mu0 = np.mean(X0, axis=0)\n",
    "        self.mu1 = np.mean(X1, axis=0)\n",
    "\n",
    "        diff = np.zeros_like(X)\n",
    "        diff[Y == 0] = X0 - self.mu0\n",
    "        diff[Y == 1] = X1 - self.mu1\n",
    "        \n",
    "        self.sigma = (diff.T @ diff) / m\n",
    "\n",
    "    def predict(self, X):\n",
    "        log_prob0 = self.log_prob(X, self.mu0, self.sigma) + np.log(1 - self.phi + self.eps)\n",
    "        log_prob1 = self.log_prob(X, self.mu1, self.sigma) + np.log(self.phi + self.eps)\n",
    "        \n",
    "        predictions = (log_prob1 > log_prob0).astype(int)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    def log_prob(self, X, mu, sigma):\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        inv_sigma = np.linalg.inv(sigma + np.eye(n_features) * self.eps)\n",
    "        det_sigma = np.linalg.det(sigma)\n",
    "        \n",
    "        log_const = -0.5 * n_features * np.log(2 * np.pi) - 0.5 * np.log(det_sigma + self.eps)\n",
    "        diff = X - mu\n",
    "        log_exp = -0.5 * np.sum(diff @ inv_sigma * diff, axis=1)\n",
    "        \n",
    "        return log_const + log_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f9606c",
   "metadata": {},
   "source": [
    "## Naive Bayes for text classification\n",
    "It will be use ful when we want to predict spam and non spam from the given messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd232dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class naive_baiyes():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, matrix, labels):\n",
    "        n, V = matrix.shape\n",
    "        y = labels\n",
    "        self.phi_y_1 = np.sum(y==1)/n\n",
    "        phi_k_given_0 = np.zeros((V,))\n",
    "        phi_k_given_1 = np.zeros((V,))\n",
    "        for i in range(n):\n",
    "            for j in range(V):\n",
    "                if y[i]==0:\n",
    "                    phi_k_given_0[j]+= (matrix[i][j])\n",
    "                else:\n",
    "                    phi_k_given_1[j]+= (matrix[i][j])\n",
    "\n",
    "        self.phi_k_given_1 = (phi_k_given_1 +1)/(np.sum(phi_k_given_1)+V)\n",
    "        self.phi_k_given_0 = (phi_k_given_0 +1)/(np.sum(phi_k_given_0)+V)\n",
    "    \n",
    "    def predict(self,matrix ):\n",
    "        sum_log_p_x_y1 = (np.log(self.phi_k_y1) * matrix).sum(axis=1) + np.log(self.phi_y)\n",
    "        sum_log_p_x_y0 = (np.log(self.phi_k_y0) * matrix).sum(axis=1) + np.log(1 - self.phi_y)\n",
    "        return (sum_log_p_x_y1 > sum_log_p_x_y0).astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
